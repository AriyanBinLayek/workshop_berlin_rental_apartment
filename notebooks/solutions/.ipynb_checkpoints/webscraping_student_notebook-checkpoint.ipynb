{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide you with an example of webscraping.\n",
    "\n",
    "Before webscraping you have to check robots.txt in order to know rules of game.\n",
    "\n",
    "**Question:** Are you allowed to webscrap wikipedia.org (German version)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract the geo locations of the Berlin U-Bahns from the wikipedia webite. Geo locations of Berlin U-Bahn can be found [here](https://de.wikipedia.org/wiki/Liste_der_Berliner_U-Bahnhöfe).\n",
    "\n",
    "1) Find the containers (classes) where the geo-location is saved on those websites. Each browser has an \"inspect\" that will show the html code of the website.\n",
    "\n",
    "2) Once you found it, you will see the link with the geo parameters, e.g: \n",
    "\n",
    "`<a href=\"//tools.wmflabs.org/geohack/geohack.php?pagename=Liste_der_Berliner_U-Bahnh%C3%B6fe&amp;language=de&amp;params=52.560555555556_N_13.334166666667_E_region:DE-BE_type:landmark&amp;title=Afrikanische+Stra%C3%9Fe\" title=\"Afrikanische Straße\"`\n",
    "\n",
    "\n",
    "3) You have to extract only following coordinates: 52.560555555556, 13.334166666667 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the page and selected item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"html_wikipedia.png\"\n",
    "Image(filename = PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # parses the html\n",
    "import requests # fetches the data from the website\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fetch content from the url\n",
    "Use requests library. Useful [functions](http://docs.python-requests.org/en/master/), [code snippes](https://gist.github.com/jkokatjuhha/02af3a28cf512ee8a3096273850fe029)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parse html\n",
    "1. Parse the request with [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "2. Find the class (which you found in the 1.1) step) in the parsed html. Useful function might be find_all() with 'class_' argument, check it [here](https://www.crummy.com/software/BeautifulSoup/bs3/documentation.html#The%20basic%20find%20method:%20findAll(name,%20attrs,%20recursive,%20text,%20limit,%20**kwargs)\n",
    "\n",
    "The result should be the array containing this specific class for each station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Find and save coordinates of each station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from 2.2 should be the array containing this specific class for each station. This means that we need to iterate over each element to extract the station coordinates and the name.\n",
    "\n",
    "Each element of your result from 2.2 will have an 'a href' element (a link) which you can also extract with a 'find' function as mentioned [here](https://stackoverflow.com/questions/5815747/beautifulsoup-getting-href)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = [] # latitude\n",
    "long = [] # longtitude\n",
    "station_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in coordinates:\n",
    "    # YOUR CODE HERE START\n",
    "    finds =\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    href = re.findall(r'(?<=params=).*?(?=_E_region)', finds['href'])[0].split(\"_N_\")\n",
    "    \n",
    "    # YOUR CODE HERE START\n",
    "    # append latitude to latitude\n",
    "    # append longtitude to longtitude\n",
    "    # append station name\n",
    "    \n",
    "    # YOUR CODE HERE END  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the lat, long and station name into a dataframe and check it out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
